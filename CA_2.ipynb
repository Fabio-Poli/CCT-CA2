{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "9f30effb",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-11-05 12:22:24,446 WARN util.NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n",
      "2023-11-05 12:22:25,554 WARN util.Utils: Service 'SparkUI' could not bind on port 4040. Attempting port 4041.\n"
     ]
    },
    {
     "ename": "Py4JError",
     "evalue": "An error occurred while calling None.org.apache.spark.sql.SparkSession. Trace:\npy4j.Py4JException: Constructor org.apache.spark.sql.SparkSession([class org.apache.spark.SparkContext, class java.util.HashMap]) does not exist\n\tat py4j.reflection.ReflectionEngine.getConstructor(ReflectionEngine.java:179)\n\tat py4j.reflection.ReflectionEngine.getConstructor(ReflectionEngine.java:196)\n\tat py4j.Gateway.invoke(Gateway.java:237)\n\tat py4j.commands.ConstructorCommand.invokeConstructor(ConstructorCommand.java:80)\n\tat py4j.commands.ConstructorCommand.execute(ConstructorCommand.java:69)\n\tat py4j.GatewayConnection.run(GatewayConnection.java:238)\n\tat java.lang.Thread.run(Thread.java:750)\n\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mPy4JError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[1], line 5\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mpyspark\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01msql\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mfunctions\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m to_timestamp\n\u001b[1;32m      4\u001b[0m \u001b[38;5;66;03m# Initialize SparkSession with legacy time parser policy\u001b[39;00m\n\u001b[0;32m----> 5\u001b[0m spark \u001b[38;5;241m=\u001b[39m \u001b[43mSparkSession\u001b[49m\u001b[43m \u001b[49m\u001b[43m\\\u001b[49m\n\u001b[1;32m      6\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbuilder\u001b[49m\u001b[43m \u001b[49m\u001b[43m\\\u001b[49m\n\u001b[1;32m      7\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mappName\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mPySparkSQL\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[43m\\\u001b[49m\n\u001b[1;32m      8\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mconfig\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mspark.sql.legacy.timeParserPolicy\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mLEGACY\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[43m\\\u001b[49m\n\u001b[1;32m      9\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgetOrCreate\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     11\u001b[0m \u001b[38;5;66;03m# Define the path to the CSV file\u001b[39;00m\n\u001b[1;32m     12\u001b[0m file_path \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m/home/hduser/Desktop/ProjectTweets.csv\u001b[39m\u001b[38;5;124m\"\u001b[39m\n",
      "File \u001b[0;32m~/miniconda3/envs/myenv/lib/python3.8/site-packages/pyspark/sql/session.py:500\u001b[0m, in \u001b[0;36mSparkSession.Builder.getOrCreate\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    497\u001b[0m     sc \u001b[38;5;241m=\u001b[39m SparkContext\u001b[38;5;241m.\u001b[39mgetOrCreate(sparkConf)\n\u001b[1;32m    498\u001b[0m     \u001b[38;5;66;03m# Do not update `SparkConf` for existing `SparkContext`, as it's shared\u001b[39;00m\n\u001b[1;32m    499\u001b[0m     \u001b[38;5;66;03m# by all sessions.\u001b[39;00m\n\u001b[0;32m--> 500\u001b[0m     session \u001b[38;5;241m=\u001b[39m \u001b[43mSparkSession\u001b[49m\u001b[43m(\u001b[49m\u001b[43msc\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moptions\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_options\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    501\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    502\u001b[0m     \u001b[38;5;28mgetattr\u001b[39m(\n\u001b[1;32m    503\u001b[0m         \u001b[38;5;28mgetattr\u001b[39m(session\u001b[38;5;241m.\u001b[39m_jvm, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mSparkSession$\u001b[39m\u001b[38;5;124m\"\u001b[39m), \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mMODULE$\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    504\u001b[0m     )\u001b[38;5;241m.\u001b[39mapplyModifiableSettings(session\u001b[38;5;241m.\u001b[39m_jsparkSession, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_options)\n",
      "File \u001b[0;32m~/miniconda3/envs/myenv/lib/python3.8/site-packages/pyspark/sql/session.py:589\u001b[0m, in \u001b[0;36mSparkSession.__init__\u001b[0;34m(self, sparkContext, jsparkSession, options)\u001b[0m\n\u001b[1;32m    585\u001b[0m         \u001b[38;5;28mgetattr\u001b[39m(\u001b[38;5;28mgetattr\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_jvm, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mSparkSession$\u001b[39m\u001b[38;5;124m\"\u001b[39m), \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mMODULE$\u001b[39m\u001b[38;5;124m\"\u001b[39m)\u001b[38;5;241m.\u001b[39mapplyModifiableSettings(\n\u001b[1;32m    586\u001b[0m             jsparkSession, options\n\u001b[1;32m    587\u001b[0m         )\n\u001b[1;32m    588\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 589\u001b[0m         jsparkSession \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_jvm\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mSparkSession\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_jsc\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msc\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moptions\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    590\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    591\u001b[0m     \u001b[38;5;28mgetattr\u001b[39m(\u001b[38;5;28mgetattr\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_jvm, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mSparkSession$\u001b[39m\u001b[38;5;124m\"\u001b[39m), \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mMODULE$\u001b[39m\u001b[38;5;124m\"\u001b[39m)\u001b[38;5;241m.\u001b[39mapplyModifiableSettings(\n\u001b[1;32m    592\u001b[0m         jsparkSession, options\n\u001b[1;32m    593\u001b[0m     )\n",
      "File \u001b[0;32m~/miniconda3/envs/myenv/lib/python3.8/site-packages/py4j/java_gateway.py:1587\u001b[0m, in \u001b[0;36mJavaClass.__call__\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m   1581\u001b[0m command \u001b[38;5;241m=\u001b[39m proto\u001b[38;5;241m.\u001b[39mCONSTRUCTOR_COMMAND_NAME \u001b[38;5;241m+\u001b[39m\\\n\u001b[1;32m   1582\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_command_header \u001b[38;5;241m+\u001b[39m\\\n\u001b[1;32m   1583\u001b[0m     args_command \u001b[38;5;241m+\u001b[39m\\\n\u001b[1;32m   1584\u001b[0m     proto\u001b[38;5;241m.\u001b[39mEND_COMMAND_PART\n\u001b[1;32m   1586\u001b[0m answer \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_gateway_client\u001b[38;5;241m.\u001b[39msend_command(command)\n\u001b[0;32m-> 1587\u001b[0m return_value \u001b[38;5;241m=\u001b[39m \u001b[43mget_return_value\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1588\u001b[0m \u001b[43m    \u001b[49m\u001b[43manswer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_gateway_client\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_fqn\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1590\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m temp_arg \u001b[38;5;129;01min\u001b[39;00m temp_args:\n\u001b[1;32m   1591\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(temp_arg, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m_detach\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n",
      "File \u001b[0;32m~/miniconda3/envs/myenv/lib/python3.8/site-packages/py4j/protocol.py:330\u001b[0m, in \u001b[0;36mget_return_value\u001b[0;34m(answer, gateway_client, target_id, name)\u001b[0m\n\u001b[1;32m    326\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m Py4JJavaError(\n\u001b[1;32m    327\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mAn error occurred while calling \u001b[39m\u001b[38;5;132;01m{0}\u001b[39;00m\u001b[38;5;132;01m{1}\u001b[39;00m\u001b[38;5;132;01m{2}\u001b[39;00m\u001b[38;5;124m.\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39m\n\u001b[1;32m    328\u001b[0m             \u001b[38;5;28mformat\u001b[39m(target_id, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m, name), value)\n\u001b[1;32m    329\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 330\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m Py4JError(\n\u001b[1;32m    331\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mAn error occurred while calling \u001b[39m\u001b[38;5;132;01m{0}\u001b[39;00m\u001b[38;5;132;01m{1}\u001b[39;00m\u001b[38;5;132;01m{2}\u001b[39;00m\u001b[38;5;124m. Trace:\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;132;01m{3}\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39m\n\u001b[1;32m    332\u001b[0m             \u001b[38;5;28mformat\u001b[39m(target_id, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m, name, value))\n\u001b[1;32m    333\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    334\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m Py4JError(\n\u001b[1;32m    335\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mAn error occurred while calling \u001b[39m\u001b[38;5;132;01m{0}\u001b[39;00m\u001b[38;5;132;01m{1}\u001b[39;00m\u001b[38;5;132;01m{2}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39m\n\u001b[1;32m    336\u001b[0m         \u001b[38;5;28mformat\u001b[39m(target_id, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m, name))\n",
      "\u001b[0;31mPy4JError\u001b[0m: An error occurred while calling None.org.apache.spark.sql.SparkSession. Trace:\npy4j.Py4JException: Constructor org.apache.spark.sql.SparkSession([class org.apache.spark.SparkContext, class java.util.HashMap]) does not exist\n\tat py4j.reflection.ReflectionEngine.getConstructor(ReflectionEngine.java:179)\n\tat py4j.reflection.ReflectionEngine.getConstructor(ReflectionEngine.java:196)\n\tat py4j.Gateway.invoke(Gateway.java:237)\n\tat py4j.commands.ConstructorCommand.invokeConstructor(ConstructorCommand.java:80)\n\tat py4j.commands.ConstructorCommand.execute(ConstructorCommand.java:69)\n\tat py4j.GatewayConnection.run(GatewayConnection.java:238)\n\tat java.lang.Thread.run(Thread.java:750)\n\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import to_timestamp\n",
    "\n",
    "# Initialize SparkSession with legacy time parser policy\n",
    "spark = SparkSession \\\n",
    "    .builder \\\n",
    "    .appName(\"PySparkSQL\") \\\n",
    "    .config(\"spark.sql.legacy.timeParserPolicy\", \"LEGACY\") \\\n",
    "    .getOrCreate()\n",
    "\n",
    "# Define the path to the CSV file\n",
    "file_path = \"/home/hduser/Desktop/ProjectTweets.csv\"\n",
    "\n",
    "# Load the CSV data into a DataFrame\n",
    "tweets_df = spark.read.format(\"csv\").option(\"header\", \"false\").option(\"inferSchema\", \"true\").load(file_path)\n",
    "\n",
    "# Rename the columns according to your CSV structure\n",
    "tweets_df = tweets_df.withColumnRenamed(\"_c0\", \"Sequencial\") \\\n",
    "                     .withColumnRenamed(\"_c1\", \"Serial_Number\") \\\n",
    "                     .withColumnRenamed(\"_c2\", \"Date\") \\\n",
    "                     .withColumnRenamed(\"_c3\", \"NO_QUERY\") \\\n",
    "                     .withColumnRenamed(\"_c4\", \"User_Name\") \\\n",
    "                     .withColumnRenamed(\"_c5\", \"Tweet\")\n",
    "\n",
    "# Use the old timestamp format that matches the legacy parser\n",
    "old_timestamp_format = \"EEE MMM dd HH:mm:ss z yyyy\"\n",
    "tweets_df = tweets_df.withColumn(\"Date\", to_timestamp(tweets_df[\"Date\"], old_timestamp_format))\n",
    "\n",
    "# Drop the unnecessary columns\n",
    "tweets_df = tweets_df.drop(\"Serial_Number\", \"NO_QUERY\", \"User_Name\")\n",
    "\n",
    "# Show the DataFrame to verify the columns are dropped\n",
    "tweets_df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be7491aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Count the total number of tweets\n",
    "tweet_count = tweets_df.count()\n",
    "\n",
    "print(f\"Total number of tweets: {tweet_count}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6fcc11d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import col\n",
    "nightmare_tweets = tweets_df.filter(col(\"Tweet\").contains(\"nightmare\"))\n",
    "nightmare_tweet_count = nightmare_tweets.count()\n",
    "print(f\"Number of tweets containing 'nightmare': {nightmare_tweet_count}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d54b5709",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import udf, col\n",
    "from pyspark.sql.types import IntegerType, StringType\n",
    "from textblob import TextBlob\n",
    "\n",
    "# Initialize SparkSession (assuming it's already started in your environment)\n",
    "spark = SparkSession.builder.appName(\"SentimentAnalysis\").getOrCreate()\n",
    "\n",
    "# Define a function to compute sentiment\n",
    "def get_tweet_sentiment(tweet_text):\n",
    "    # Use TextBlob to get the sentiment polarity\n",
    "    analysis = TextBlob(tweet_text)\n",
    "    # Set threshold for positive and negative sentiments\n",
    "    if analysis.sentiment.polarity > 0:\n",
    "        return 1\n",
    "    elif analysis.sentiment.polarity == 0:\n",
    "        return 0\n",
    "    else:\n",
    "        return -1\n",
    "\n",
    "# Define the function to truncate the tweet text for better display\n",
    "def truncate_string(s, length=50):\n",
    "    return s if len(s) <= length else s[:length-3] + '...'\n",
    "\n",
    "# Register the UDFs\n",
    "sentiment_udf = udf(get_tweet_sentiment, IntegerType())\n",
    "truncate_udf = udf(truncate_string, StringType())\n",
    "\n",
    "# Apply the sentiment UDF to the DataFrame\n",
    "tweets_df = tweets_df.withColumn('SentimentScore', sentiment_udf(col('Tweet')))\n",
    "\n",
    "# Apply the truncation UDF to the DataFrame to shorten the tweet text\n",
    "tweets_df = tweets_df.withColumn('TruncatedTweet', truncate_udf(col('Tweet')))\n",
    "\n",
    "# Select the desired columns to show and display the DataFrame as a table, excluding 'Sequencial'\n",
    "tweets_df.select('SentimentScore', 'Date', 'TruncatedTweet').show(truncate=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "65e9c94d",
   "metadata": {},
   "source": [
    "## Create a Average Sentimental by Day"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce206d94",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import functions as F\n",
    "\n",
    "# Group by the date and calculate the average sentiment score, ensure the grouped date column has a simple alias\n",
    "time_series_df = tweets_df.groupBy(F.to_date(\"Date\").alias(\"Date\")).agg(F.avg(\"SentimentScore\").alias(\"AvgSentiment\"))\n",
    "\n",
    "# Sort the data by the aliased date\n",
    "time_series_df = time_series_df.orderBy(\"Date\")\n",
    "\n",
    "# Show the DataFrame to verify the time series data\n",
    "time_series_df.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "12a06324",
   "metadata": {},
   "source": [
    "### Given the nature of the dataset (sentiment scores over time), you might start with LSTM to capture potential non-linear patterns. Then, you could use ARIMA as a comparison to see how a more traditional linear method performs on your data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "07056822",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Assuming you have the PySpark DataFrame named `time_series_df`\n",
    "pandas_df = time_series_df.toPandas()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a69b2de2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the DataFrame to a CSV file\n",
    "pandas_df.to_csv('/home/hduser/Desktop/tweet_sentiment_time_series.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "32b8dbd6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Load the dataset\n",
    "dataset = pd.read_csv('/home/hduser/Desktop/tweet_sentiment_time_series.csv')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df59b7ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "import datetime as dt  # Import the datetime module\n",
    "\n",
    "# Assuming 'dataset' is a pandas DataFrame that includes 'Date' and 'AvgSentiment'\n",
    "# Convert date to ordinal to represent it numerically\n",
    "dataset['Date'] = pd.to_datetime(dataset['Date']).map(dt.datetime.toordinal)\n",
    "\n",
    "# Scale the AvgSentiment values\n",
    "scaler = MinMaxScaler(feature_range=(0, 1))\n",
    "scaled_data = scaler.fit_transform(dataset['AvgSentiment'].values.reshape(-1, 1))\n",
    "\n",
    "# Create the time steps for LSTM (let's assume we're using 5 days to predict the next day)\n",
    "def create_dataset(data, time_step=1):\n",
    "    X, y = [], []\n",
    "    for i in range(len(data) - time_step - 1):\n",
    "        a = data[i:(i + time_step), 0]\n",
    "        X.append(a)\n",
    "        y.append(data[i + time_step, 0])\n",
    "    return np.array(X), np.array(y)\n",
    "\n",
    "time_step = 5\n",
    "X, y = create_dataset(scaled_data, time_step)\n",
    "\n",
    "# The rest of your LSTM code would go here\n",
    "# ...\n",
    "\n",
    "# Make sure to reshape 'X' to be [samples, time steps, features] which is required for LSTM\n",
    "X = X.reshape(X.shape[0], X.shape[1], 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6cb3f9a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, LSTM\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Split the data into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Define the LSTM model architecture\n",
    "model = Sequential()\n",
    "model.add(LSTM(units=50, return_sequences=True, input_shape=(X_train.shape[1], 1)))\n",
    "model.add(LSTM(units=50, return_sequences=False))\n",
    "model.add(Dense(units=25))\n",
    "model.add(Dense(units=1))\n",
    "\n",
    "# Compile the model\n",
    "model.compile(optimizer='adam', loss='mean_squared_error')\n",
    "\n",
    "# Train the model\n",
    "model.fit(X_train, y_train, batch_size=1, epochs=1)  # You can adjust the batch size and epochs\n",
    "\n",
    "# Evaluate the model\n",
    "testing_loss = model.evaluate(X_test, y_test)\n",
    "\n",
    "# Forecasting\n",
    "# You'll need to prepare your input data similarly to how you created your X dataset,\n",
    "# and then you can use model.predict() to make your forecasts.\n",
    "\n",
    "# After making predictions, you need to rescale them back to the original range\n",
    "# This is just a placeholder for the prediction phase\n",
    "# predictions = model.predict(X_test)\n",
    "# predictions = scaler.inverse_transform(predictions)\n",
    "\n",
    "# It's important to remember that you'll need to adjust hyperparameters like the number of LSTM units, \n",
    "# batch size, and epochs based on your specific dataset and the results you observe.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "68010b18",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a9759651",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cecf38bd",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "93909bc1",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bbe158c3",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d00fb65e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1fc7b3e8",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
